{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPAM Systemas, great company"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epam: (IPAM)\n",
    "\n",
    "- Casa desarrollo software\n",
    "- empresa publica norteamerciana\n",
    "- cotizan  en la bolsa en newyork\n",
    "- Ellos son partner oficiales\n",
    "- 100% cloud \n",
    "- Equiṕos de n personas\n",
    "- proyectos que duran de 3 a 18 meses.\n",
    "- Equipos globales : China, Mexico, Canada\n",
    "- La solución es a clientes americanos\n",
    "- Lunes a Vieres 40 horas a la semana\n",
    "- No hay revisión de tracking diario\n",
    "- Reuniones con el cliente y deliverys\n",
    "- Cada 6 meses evaluación de prpmoción por desempeño o nivelación salarial, se hace reajuste.\n",
    "- Se paga por skils y conocimiento, no por años de experiencia\n",
    "- PRe pagaa para pareja y niños\n",
    "- bonos por deseméño \n",
    "- computdor\n",
    "- equipo ergnomico se puede solicitar\n",
    "- Posibilidad de relocation dentro de las 45 compañias, pago de visado, primer mes de renta. Se han - hecho más de 1000 relocations.\n",
    "- Programa de referidos, acceso a compra de acciones con EPAM con 15% descuento\n",
    "pago COP\n",
    "\n",
    "\n",
    "\n",
    "- B1 -> Perfil semi senior -> 16 millones.\n",
    "- Están contratando senior B2, a nivel de perfil.\n",
    "- Más adelante para roles semi senior\n",
    "- 12 preguntas conceptuales . Enviar la prueba para semi senior para avanzar. 65% o superior. Alvaro Pabón. En unos meses bases de datos. I got 86%\n",
    "- B2B goal: Crean muchos modelos y llegar en live. El pricing. Se logró bajar el bad rate en un 14%. B2B. \n",
    "\n",
    "\n",
    "# TODO:\n",
    "\n",
    "* Start with learning Spark and other technologies required by the selection\n",
    "* Dont forget talk about success models: 2% bad rate we achieve about 3 or 4%\n",
    "* Machine Learning technologies: KubeFlow, SageMaker, Argo, MlFlow\n",
    "* Library - practice to larga datasets Dask\n",
    "* Add to this project Mo codes enclosing files and more data.\n",
    "\n",
    "\n",
    "### Questions for EPAM Systems:\n",
    "\n",
    "1. Adaboost: how perform this model\n",
    "2. Random forest (twice): What hyper parameters causes Overfitting ?: Me depth\n",
    "3. Which of the following models apply Early stopping: Gradient boosting, random forest, SVM, KNN\n",
    "\n",
    "    - All explanation at https://towardsdatascience.com/a-practical-introduction-to-early-stopping-in-machine-learning-550ac88bc8fd\n",
    "    - Early stopping support in Gradient Boosting enables us to find the least number of iterations which is sufficient to build a model that generalizes well to unseen data.\n",
    "\n",
    "4. To be clear number of layers and number of neurons total\n",
    "5.  If set depth=100 in Random forest hyper parameter...what happened? : Me, high variance and low bais: Overfitting.\n",
    "\n",
    "7. K-means: Which is True: Me: The seed applied change the results.\n",
    "\n",
    "    - Yeah, becauze initialize clusters centers (randomly, dependes the seed)\n",
    "    - What is the silhouette_score: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#:~:text=The%20Silhouette%20Coefficient%20is%20calculated,is%20not%20a%20part%20of.\n",
    "    - \n",
    "\n",
    "8. Feature scaling, what is true....Me I selected Improve less time processing in training process.\n",
    "9. One hot encoding problem: Me selected tha SOme instance in test data dont exists in training data.\n",
    "\n",
    "10. How to control overfitting: \n",
    "    - Cross validation and at last select sub set to test\n",
    "    - Avoid deep trees (If model is a kinf of tree or ensembled)\n",
    "    - Reduce the number of features. (Maybe PCA strategy)\n",
    "    - Increase the number of training samples\n",
    "    - To apply early stopping as a regularization technique to combat overfitting.\n",
    "    - Regularization: It is a technique that keeps all the features but reduces the magnitude of their effects by imposing a penalty on the \n",
    "       features which do not impact the target much. Some regulariation techiques (like ridge/L2 regularization) \n",
    "       sets the coefficient of features to a small values while others (like LASSO/L1 regularization) technique sets the coefficient of features to exactly zero.\n",
    "\n",
    "11. **how to address imbalance datasets ?**\n",
    "\n",
    "    - Use the right evaluation metrics: Precision, recall and F1, Furthermore,  MCC (Mathews' correlation coeficient) or phi statistic. Its a measure of the asociation between 2 binary variables, AUC: Relation between true positive and false positive rate.\n",
    "    - Resample the training set: Under-sampling: reducing the size of the abundant class. Apply with large datasets. Over-sampling: Increasing the size of rare samples. These new rare samples are generated using: repetition, bootstrapping, SMOTE.\n",
    "    - According with https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html the out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class.\n",
    "    - If We are using TensorFlow, class_weigh could be a easy way. https://towardsdatascience.com/dealing-with-imbalanced-data-in-tensorflow-class-weights-60f876911f99. Here, model could pay more attention to examples where the loan was defaulted. In binary classification, class weight could be represented just by calculating the frequency of the positive and negative class and then inverting it so that when multiplied to the class loss, the underrrepresnted class has a much higher error than the majority class.\n",
    "    - XGBoost is a robust model to address this complexity.\n",
    "\n",
    "12. How to fill NaN values:\n",
    "    - When column is a numeric field, can use the mean or median. Otherwise zero.\n",
    "    - When column is a categorical field, can use filling with the mode or top fequence of the value.\n",
    "    - Use fillna(value) combined with lambda solution.\n",
    "    - Imputation with Imputing categorial.\n",
    "    \n",
    "13. Which are NOT robust models to NaN values ?\n",
    "    - SVM or Support Vector Machine and neural networks cannot tolerate missing values.\n",
    "    - Tree-based models have clever to incomplete data.\n",
    "    - \n",
    "    - \n",
    "    - \n",
    "15. LDA for clusters from scratch\n",
    "    - LDA or PCA ?    \n",
    "16. K-prototypes\n",
    "    - Complete this.\n",
    "16. B2B solutions: saving millions of dollars each year through infusing machine learning into contact operations, using our intelligent resolution recommendation systems like Dynamic Discount to Keep model which uses AI to offer a fair discount to customers in post delivery incidents.\n",
    "17.  A/B testing and causal inference techniques ?\n",
    "    - A/B Testing are tests over new and old models with some changis and after can evaluate how to improve the KPIs on a company.\n",
    "    - Sagemake allows to distribuite or address streams according with test users.\n",
    "    \n",
    "    \n",
    "# You have to read: http://www.feat.engineering/visualizations-for-categorical-data-exploring-the-okcupid-data.html\n",
    "\n",
    "![Alt text](EPAM_codility_Testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test int smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smallest number. If not, default is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(A):\n",
    "    # write your code in Python 3.6\n",
    "    smallest = 1\n",
    "    for n in A:\n",
    "        if smallest in A:\n",
    "            smallest += 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " A = [-1,-3,1,2,5,4,3,6,8]\n",
    "solution(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker: \n",
    "\n",
    "* Environment of Machine Learning to create datapipelines from data to production\n",
    "* Create Notebooks and instances with pre created models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "https://www.youtube.com/watch?v=LsK-xG1cLYA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Boosting creates several simple predictors in sequence in such a way the second adjust good that first one couldn't and so on.\n",
    "\n",
    "- Appear a new term: *stumps* In a forest of trees made with AdaBoost the trees usually just a node and 2 leaves. Forest of stumps rather than trees.\n",
    "- Larger stumps will have more say in the final decision \n",
    "- A difference with Random forest with independ trees, in a Forest of stumps made with Adaboost order is important. In this way, the error is sequence of the next stump.\n",
    "- AdaBoost combine weak learners (stumps) to make classifications\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](adaboost_1.png)![Alt text](adaboost_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
