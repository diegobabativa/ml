{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f66aafa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "1. **As an hyperparameter of the gradient algorithm, what is the influence of the learning rate on this algorithm?**\n",
    "\n",
    " - Learning rate controls how big a step we should take after each iteration.\n",
    " - \"learning rate\" slows convergence down if its value is too low\n",
    " - \"learning rate\" increases risks of non-convergence if its value is too high\n",
    "     \n",
    "    \n",
    "     \n",
    "2. **Computing Euclidean and Cosine** distance\n",
    "\n",
    "-  compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\n",
    "    python answer: np.dot(v1,v2) / np.linal.norm(v1) * np.linal.norm(v2) \n",
    "    \n",
    "- Compute euclidean distance dist(x, y) = sqrt(v1 dot v1 - 2 * v1 dot v2 + v2 dot v2)\n",
    "    python answer: sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f31d777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Sklearn:  [[0.29291923]]\n",
      "Cosine Diego  :  0.29291923109788465\n",
      "Euclidean Sklean  :  [[25.59296778]]\n",
      "Eucliean Diego  :  25.592967784139454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "vector1 = np.array([15,5,7,1,1,0,0,1])\n",
    "vector2 = np.array([1,5,7,8,8,0,0,20])\n",
    "\n",
    "\n",
    "def cosine(vector1, vector2):\n",
    "    return  np.dot(vector1,vector2) / (np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "\n",
    "\n",
    "def euclidean(vector1, vector2):\n",
    "    return np.sqrt(np.dot(vector1, vector1) - 2* np.dot(vector1, vector2)+ np.dot(vector2, vector2))\n",
    "\n",
    "print(\"Cosine Sklearn: \", cosine_similarity([vector1], [vector2]))\n",
    "print(\"Cosine Diego  : \", cosine(vector1, vector2))\n",
    "print(\"Euclidean Sklean  : \", euclidean_distances([vector1], [vector2]))\n",
    "print(\"Eucliean Diego  : \", euclidean(vector1, vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f68aa",
   "metadata": {},
   "source": [
    "\n",
    "## 1.  Introduction to Neural Networks:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/\n",
    "\n",
    "        \n",
    "**Activation functions**: \n",
    "    \n",
    "        - RELu: Is one of the most used in neural networks. Updates te parameter much faster.\n",
    "        - ELU\n",
    "        - Sigmoid\n",
    "        - Tangente hiperbolica\n",
    "        - logistic regression\n",
    "        - We need NON-LIENAR ACTIVATION functions for neural networks\n",
    "        \n",
    "    \n",
    "   **Cost functions**: Used by find the parameters that the output or predicted values are very close to real target. \n",
    "        \n",
    "    - Home features --> stimated price     --> Standar neural network\n",
    "    - Ad, user      --> click prediction   --> Standar neural network\n",
    "    - Image         --> Image class        --> CNN \n",
    "    - Audio         --> Text Transctip     --> RNN\n",
    "    - English       --> Machine Translation--> RNN\n",
    "    \n",
    "   **Gradient Descent**: This is a techinc to helps to learn the parameters W and b in cuch a way that the cot function is mnimized. Steps: 1. Initialize w and b, 2. Take step in the steepest downhill diretion, and 3. Repeat step 2 until global optimun is achieved.\n",
    "   \n",
    "   - w:= w-alpha dJ(w,b)/ dw\n",
    "   - b:= b-alpha dJ(w,b)/ db\n",
    " \n",
    " \n",
    "**Neural Network representation**\n",
    "   \n",
    "   ![Alt text](NN.png)\n",
    "   \n",
    "   - **Random initialization**: the weights generally are random initialized with small weights. \n",
    "   \n",
    "   \n",
    " **Back progpagation**: Backpropagation uses a methodology called chain rule to improve outputs. Basically, after each forward pass through a network, the algorithm performs a backward pass to adjust the model’s weights. The term is sometimes used as a synonym for \"error correction.\"\n",
    "\n",
    "\n",
    " - **Parameters:** Are learned by the model during the training time. PArameters in a deep neural network  are W and B which the model updates during the backpropagation step.\n",
    " - **hyperparameters:** Can be change before trainig the model. \n",
    "         - Learning rate, \n",
    "         - Number of iterations\n",
    "         - Number of hidden layers\n",
    "         - Units in each hidden layer\n",
    "         - Choice of activation functions\n",
    "         \n",
    "         \n",
    "   ## 2.  Improving Neural Networks – Hyperparameter Tuning, Regularization, and More (deeplearning.ai Course #2)\n",
    "   \n",
    "https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/\n",
    "\n",
    "Training: 60%, evaluation: 20% and Test 20%.\n",
    "\n",
    "   - high bias: The model is very simple (strainght line) to separte de classes.\n",
    "   - High variance: The model over fit with the training data.\n",
    "    - The just right is low bias and low variance.\n",
    "    - If the model has high bias, try deep models train model for a long period of time.\n",
    "    - If the model has high variance, try with more data, use regularization or other neural network architectures. Apply some kind of regularization.\n",
    "    - **Regularization**: L2 regulariation. This is a parameter which we can tune while training the model. L2 regularization is also known as weight decay. One way to prevent overfitting is to reduce the complexity of the model. \n",
    "    - **Dropout regularization** If We set dropout in 0.5, The model randomly remove 50% of the units from each layer. Dropout adds noise to the predictions\n",
    "    - Early stopping: Method to avoid overfitting\n",
    "    - Normalizing data make the algorithm faster.\n",
    "    - **Dealing with Vanishing**: weights will make the gradients very small\n",
    "    - **Exploding gradients:** L will be large, making the gradients very large and the learning process slow\n",
    "    - **Optimization Algorithms:**  \n",
    "    - Stochastic GRadient Descent\n",
    "    - **Optimizers**: Help to global minima of a cost function faster and hence reduce the learning time.\n",
    "    - **Momentum**: Uses an Alpha and Beta. Alpha controls speed in the gradient and Beta is the friction.\n",
    "    - RMSProp\n",
    "    - **Adam**:  Is the combination of momentum and RMSprop.\n",
    "    - Mini-batch Gradient Descent: To accelerate the convergence and improve the optimization. Specify littles portions of data to converge the algorithm.\n",
    "    - **Learning rate Decay**: Dynamic change of the learning rate.\n",
    "    - **Multi-class classification:** Softmax activation function Regression: \n",
    "    \n",
    "    \n",
    "### Top 90 Data science Interview Questions and answers:\n",
    "\n",
    "https://www.simplilearn.com/tutorials/data-science-tutorial/data-science-interview-questions\n",
    "    \n",
    "   - Dont copy here, follow the link and Watch the video.\n",
    "   - **Entropy** is the degree of uncertainty, impurity or disorder of a random variable, or a measure of purity. It characterizes the impurity of an arbitrary class of examples. Here, if all elements belong to a single class, then it is termed as “Pure”, and if not then the distribution is named as “Impurity”. It is computed between 0 and 1. \n",
    "    - **Information gain** is used for determining the best features/attributes that render maximum information about a class. It follows the concept of entropy while aiming at decreasing the level of entropy, beginning from the root node to the leaf nodes. Information gain computes the difference between entropy before and after split and specifies the impurity in class elements. \n",
    "    - **Information Gain = Entropy before splitting - Entropy after splitting**\n",
    "    - **Avoid overfitting:** Keep model simpple: Take into account fewer variables, removing some noise in the training data. Use cross validation and Useregularization techniques cuch as LASSO that penalizes certain model parameters.\n",
    "    - Univariate analysis: heigh of students, mean, min, mas, mode, disprescion\n",
    "    - Bivariate analysis:  Figure out some type of relationship or correlation.\n",
    "    - Multiariate analysis: Mean, dispersion, range, minimun, median.\n",
    " \n",
    " 7. Methods for feature selection:\n",
    "     - Filter methods: LDA, ANOVA and Chi-square\n",
    "     - Wrapper methods: Forward selection, Backward selection, recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca96673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Prints number from 1 to 50. Multiples of 3 print \"Fizz\" instead of number and multiples of 5 \n",
    "# print \"Buzz\". Are multiple print \"FizzBuzz\"\n",
    "\n",
    "for number in range(51):\n",
    "    \n",
    "    if number%3 == 0 and number%5==0:\n",
    "        print(\"FizzBuzz\")\n",
    "        continue\n",
    "         \n",
    "    if number%3 == 0:\n",
    "        print(\"Fizz\")\n",
    "        continue\n",
    "        \n",
    "    if number%5==0:\n",
    "        print(\"Buzz\")\n",
    "        continue\n",
    "        \n",
    "    print(number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d5b3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "# 10. **Calculating euclidean Distance in python** given data points:\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    return np.sqrt((vector1[0]-vector2[0])**2 + (vector1[1]-vector2[1])**2)\n",
    "\n",
    "print(euclidean_distance([1,3], [2,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b787892",
   "metadata": {},
   "source": [
    "11. What is the angle between the hour and minute hands of a clock when the time is half past six ? : 15 grados\n",
    "\n",
    "15. RMSE: Root Mean Square Error\n",
    "16. MSE: Indicates the Mean Square Error\n",
    "17. If it rains on Saturday with prob 0.6 and Sundays 0.2 what is the probability rains this weekend ?: \n",
    "    Total probability - (prob not rain on Saturday) interset(Prob not rain sunday)\n",
    "    1-(1-0.6)*(1-0.2)=0.68\n",
    "\n",
    "18. How can you select K for k-means?: Elbow method. Wath is the flexion of the elbow.\n",
    "19. p-value: <= 0.05  Indicating strong evidence against the null hypothesis. Reject the null hypothesis.\n",
    "    p-value: >= 0.05: Indicates weak evidence against the null hypothesis.\n",
    "18. Outliers: drop outliers values only if it is a garbage value. If the value is extreme. If the data cannot be dropped, intent normalizing. Use algorithms than are robust for outliers, such as RANDOM FOREST.\n",
    "19. Time series data is stationary ? \n",
    "\n",
    "  ![Alt text](time_series_stationary.png)\n",
    "\n",
    " ![Alt text](precision_confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997137d",
   "metadata": {},
   "source": [
    "24. For imbalance dataset, not use accuracy, You should use Precision, Recall o F1 over minority class\n",
    "25. The followings algorithms can be used for imputing missoin values of categorical and continues variables: \n",
    "    - K - Nearest Neighborhood\n",
    "26. What is the entropy of the target variable [0,0,0,1,1,1,1,1] ? \n",
    "    - -(5/8 log(5/8) + 3/8 log(3/8))\n",
    "27. Want to predict propability of death from hear desease based on 3 facots: age, gender, blood colesterol: Logistic regresion\n",
    "28. Determine the behavior of the customers grooping for their values: K-means clustering\n",
    "30. We have website and visitors receive cupons, Visitors yo website han any impact to purchase aalysis. : ANOVA One-way\n",
    "\n",
    "\n",
    "#### Another website with questions for interview:\n",
    "https://www.edureka.co/blog/interview-questions/data-science-interview-questions/\n",
    "\n",
    "   -**Bias** : Bias is an error introduced in your model due to the oversimplification of the machine learning algorithm. It can lead to underfitting. When you train your model at that time model makes simplified assumptions to make the target function easier to understand.\n",
    "   - low bias machine learning algorithms:  Decision Trees, k-NN and SVM \n",
    "   - High bias machine learning algorithms: Linear Regression, Logistic Regression\n",
    "\n",
    " **Variance:**: Variance is error introduced in your model due to a complex machine learning algorithm, your model learns noise also from the training data set and performs badly on test data set. It can lead to high sensitivity and overfitting.\n",
    "    \n",
    "   - **Bias-Variance trade-off**: The goal of any supervised machine learning algorithm is to have low bias and low variance to achieve good prediction performance.\n",
    "\n",
    "- The k-nearest neighbour algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbours that contribute to the prediction and in turn increases the bias of the model.\n",
    "\n",
    "- The support vector machine algorithm has low bias and high variance, but the trade-off can be changed by increasing the C parameter that influences the number of violations of the margin allowed in the training data which increases the bias but decreases the variance.\n",
    "\n",
    "- There is no escaping the relationship between bias and variance in machine learning. Increasing the bias will decrease the variance. Increasing the variance will decrease bias.\n",
    "\n",
    "\n",
    "31. MArkov chain: Markov chain is a type of stochastic process. In Markov chains, the future probability of any state depends only on the current state. An example can be word recommendation. When we type a paragraph, the next word is suggested by the model which depends only on the previous word and not on anything before it.\n",
    "32. What is the ROC curve:  The ROC curve is a graph between False positive rate on the x axis and True positive rate on the y axis. True positive rate is the ratio of True positives to the total number of positive samples. False positive rate is the ratio of False positives to the total number of negative samples. The FPR and TPR are plotted on several threshold values to construct the ROC curve. The area under the ROC curve ranges from 0 to 1. A completely random model has an ROC of 0.5, which is represented by a straight line. The more the ROC curve deviates from this straight line, the better the model is. ROC curves are used for binary classification. The below image shows an example of an ROC curve. \n",
    "\n",
    " ![Alt text](ROC_AUC.png)\n",
    "\n",
    "\n",
    "33. **L2 regularization-(Ridge regression)**– In L2 regularization, we add the sum of the squares of all the weights, multiplied by a value lambda, to the loss function. The formula for Ridge regression is as follows-\n",
    "\n",
    "34. **L1 Regularization-(Lasso regression)**– In L1 regularization, we add the sum of the absolute values of all the weights, multiplied by a value lambda, to the loss function. The formula for Lasso regression is as follows-\n",
    "\n",
    "35.  **How should you maintain a deployed model?** : After a model has been deployed, it needs to be maintained. The data being fed may change over time. For example, in the case of a model predicting house prices, the prices of houses may rise over time or fluctuate due to some other factor. The accuracy of the model on new data can be recorded. Some common ways to ensure accuracy include.  The model should be frequently checked by feeding negative test data. If the model gives low accuracy with negative test data, it is fine. An Auto Encoder should be built that Using anomaly detection techniques, the AE model will calculate the Reconstruction error value. If the Reconstruction error value is high, it means the new data does not follow the old pattern learned by the model.\n",
    "\n",
    "36.  What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning?\n",
    "    - **Epoch** – Represents one iteration over the entire dataset (everything put into the training model).\n",
    "\n",
    "    - **Batch** – Refers to when we cannot pass the entire dataset into the neural network at once, so we divide the dataset into several batches.\n",
    "\n",
    "    - **Iteration** – if we have 10,000 images as data and a batch size of 200. then an epoch should run 50 iterations (10,000 divided by 50).\n",
    "    \n",
    " 37. Sklearn cheat sheet: https://www.edureka.co/blog/cheatsheets/python-scikit-learn-cheat-sheet/\n",
    " 38. Read all the questions again from this last web page, very well explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9afe98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.12435565298214\n"
     ]
    }
   ],
   "source": [
    "### Euclidean distance between 2 arrays in Nupy\n",
    "A = np.array([1,2,3])\n",
    "B = np.array([8,9,10])\n",
    "\n",
    "print(np.linalg.norm(A-B)) #Simplest way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee09a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {'bob'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a00b74",
   "metadata": {},
   "source": [
    "37. GANs unsupervised networks ?\n",
    "38. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25567f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ddf32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "752fc708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6799999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc6964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b49981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3. Kernel and identity matrix\n",
    "5. See Keras and some examples or check Javier Notebooks.\n",
    "6. Need check Javier or Tensorflows examples online, dont matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97858c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1edda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c55429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6e299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6524a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a7a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
