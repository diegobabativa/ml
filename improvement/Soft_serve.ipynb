{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cf060f",
   "metadata": {},
   "source": [
    "#### SoftServe form\n",
    "\n",
    "\n",
    " 1. **Qué es el análisis prescriptivo?**\n",
    " \n",
    "     Es aquel que brinda apoyo para que un gerente o steak holders comprandan cómo hacerlo para tener resultados eficientes en el futuro. A diferencia del análisis predictivo, le dice lo que es probable que suceda, no aconseja cuál es la mejor opción.\n",
    "     Un ejemplo es que Daviplata se da cuenta que un cliente está dismiuyendo el usao de la billetera.  Este análisis puede sugerir que se briden nuevas soluciones o se quiten algunas restricciones para evitar la pérdida del cliente.\n",
    "     \n",
    " 2. **Bayesian Inference**\n",
    " \n",
    "     Basado en la evidencia. Está soportado en el teorema de Bayes.  Basado en los eventos. A esto se le conoce como la probabilidad condicional.  Un ejemplo de inferencia bayesiana es el siguiente: Durante miles de millones de años, el sol ha salido después de haberse puesto. El sol se ha puesto esta noche. Hay una probabilidad muy alta de (o 'Yo creo firmemente' o 'es verdad') que el sol va a volver a salir mañana. \n",
    "     \n",
    " 3. **Estimators, Bias, and Variance**\n",
    " \n",
    "     In statistics, the bias of an estimator (or bias function) is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. In statistics, \"bias\" is an objective property of an estimator.\n",
    "\n",
    "4. **Bootstrap / Bagging / Ensembles of Models**\n",
    "\n",
    "    The main purpose of using an ensemble model is to group a set of weak learners and form a strong learner. The way it is done is defined in the two techniques: Bagging and Boosting that work differently and are used interchangeably for obtaining better outcomes with high precision and accuracy and fewer errors. With ensemble methods, multiple models are brought together to produce a powerful model.\n",
    "    \n",
    "    The ensemble is a method used in the machine learning algorithm. In this method, multiple models or ‘weak learners’ are trained to rectify the same problem and integrated to gain desired results. Weak models combined rightly give accurate models.\n",
    "\n",
    "    - **Bagging**: is a homogeneous weak learners’ model that learns from each other independently in parallel and combines them for determining the model average. Random forest uses bagging process.Bagging is an acronym for \"Bootsrtap Aggregation\" and is used to drecrease the variance in the prediction model. For this reason, Randome Forest is applied to avoid overfitting on decision trees models. Bagging is a parallel method that fits different, considered learners independently from each other making ir possible to train them siumstaneously. Bagging generates aditional data for training from dataset. This is achieved byrandom sampling with replacement from the original dataset. Sampling with replacement may repeat some observations in each new training data set. Every element in Bagging is equally probable for appearing in a new dataset. These multi datasets are used to train multiple models in parallel. The average of all the predictions from different ensemble models is calculated. The majority vote gained from the voting mechanism is considered when classification is made. Bagging decreases the variance and tunes the prediction to an expected outcome. Same explained in EPAM Nptebook. **Example of Bagging**: The **Random Forest** model uses Bagging, where decision tree models with higher variance are present. It makes random feature selection to grow trees. Several random trees make a Random Forest. \n",
    " <br>\n",
    "\n",
    "    - **Boosting**: Boosting is a sequential ensemble method that iteratively adjusts the weight of observation as per the last classification. If an observation is incorrectly classified, it increases the weight of that observation. The term ‘Boosting’ in a layman language, refers to algorithms that convert a weak learner to a stronger one. **It decreases the bias error** and builds strong predictive models. Strong model, **XGBoost** is used when **underfitting** appears. Data points mispredicted in each iteration are spotted, and their weights are increased. The Boosting algorithm allocates weights to each resulting model during training. A learner with good training data prediction results will be assigned a higher weight. When evaluating a new learner, Boosting keeps track of learner’s errors. **Example of boosting:** The **AdaBoost** uses Boosting techniques, where a 50% less error is required to maintain the model. Here, Boosting can keep or discard a single learner. Otherwise, the iteration is repeated until achieving a better learne and XGBoost. <br> <br>\n",
    "    \n",
    "    - **Similarities & differences**\n",
    "    - **Bagging and Boosting** are ensemble methods focused on getting N learners from a single learner.\n",
    "    - **Bagging** decreases variance, not bias, and solves over-fitting issues in a model. **Boosting** decreases bias, not variance.\n",
    "    - **In Bagging**, each model receives an equal weight. **In Boosting**, models are weighed based on their performance.\n",
    "    - Models are built independently in **Bagging**. New models are affected by a previously built model’s performance in **Boosting**.\n",
    "    \n",
    "<br> \n",
    "    \n",
    "5. Computer Vision and Natural Language Processing\n",
    "    - Convolutional Networks\n",
    "    - Recurrent Networks\n",
    "    - Deep Generative Models\n",
    "    - Generative Adversarial Network\n",
    "    - Autoencoders\n",
    "    - Structured Probabilistic Models\n",
    "    - Transformers\n",
    "    - Regularization for NN\n",
    "    - Optimization for Training NN\n",
    "    - Multi GPU Training (Thesis)\n",
    "    - TPU Training       (Thesis)\n",
    "    - Multiclass Text Classification\n",
    "    - Named Entity Recognition\n",
    "    - Pretrained Text Embeddings\n",
    "    - FastAPI / Flask / Gunicorn\n",
    "    - Visualizations (Matplotlib, Altair (maps), Plotly Dash, Streamlit, etc.)\n",
    "    - Tidyverse\n",
    "    - SQL Alchemy (It's a HQL in Java but for Python)\n",
    "    - R ML Stack (Tidyverse, data.table, Caret, etc.)\n",
    "\n",
    "\n",
    "6. Engineering Skills. Data Engineering, Clouds, DevOps.\n",
    "\n",
    "    - Apache Hadoop\n",
    "    - Apache Spark / Databricks\n",
    "    - Apache Kafka\n",
    "    - Apache Beam\n",
    "    - Apache Airflow\n",
    "    - Apache Flink\n",
    "    <br>\n",
    "    \n",
    "**Google Cloud Platfform:** <br>\n",
    "    \n",
    "    - Vertex AI (https://www.youtube.com/playlist?list=PLISuMnTdVU-xnVCQu0WXDAcvXu-PIe5f1)\n",
    "    - BigQuery\n",
    "    - Google Kubernetes Engine\n",
    "    - Cloud Run / Cloud Functions / App Engine\n",
    "    - Cloud Build\n",
    "    - PubSub\n",
    "    - Dataflow / Dataproc\n",
    "    - Cloud Composer\n",
    "    - Dialogflow\n",
    "    - Cloud IoT Core \n",
    "    \n",
    " **Assess your AWS Cloud Platform experience:**\n",
    " \n",
    "     - Lambda\n",
    "     - Sagemaker\n",
    "     - EKS / ECS / Fargate\n",
    "     - Glue / Athena / EMR\n",
    "     - SQS / SNS / Kafka\n",
    "     - Redshift\n",
    "     - Batch\n",
    "     - Kinesis\n",
    "     - AWS IoT Greengrass\n",
    "     - Cloud Formation / Step Functions\n",
    "\n",
    "**Assess your Microsoft Azure Cloud Platform experience:**\n",
    "\n",
    "    - Azure Machine Learning\n",
    "    - Azure Databricks\n",
    "    - Azure Functions\n",
    "    - AKS\n",
    "    - Batch\n",
    "    - HDInsight\n",
    "    - Event Hub\n",
    "    - Azure Pipelines\n",
    "    - Azure IoT Hub / Time-Series Insights\n",
    "    - Azure Synapse\n",
    "    - Azure Stream Analytics\n",
    "    - Azure DataFactory\n",
    "    \n",
    "    \n",
    "**Assess your DevOps technologies experience:**\n",
    "\n",
    "    - Jenkins / CircleCI / GitHub Actions\n",
    "    - Terraform\n",
    "    - Kubernetes / Openshift\n",
    "    - ArgoCD\n",
    "    - Kubeflow\n",
    "    - MLflow / DVC\n",
    "    - Deployment Strategies\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1531d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
